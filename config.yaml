# =============================================================================
# Bear Blog Automation - Central Configuration
# =============================================================================
# Adjust these values when forking this repository.
# For feed configuration, edit bots/social_bot/config.json
# =============================================================================

blog:
  # Your Bear Blog username (used for backup export URL)
  bearblog_username: "fischr"
  # Public base URL for your blog (used for link checker reports)
  site_url: "https://fischr.org"

backup:
  # Directory name for blog post backups (relative to repository root)
  folder: "blog-backup"
  # Save debug CSV file (last_export.csv) for troubleshooting
  # WARNING: The CSV contains ALL articles including unpublished drafts!
  # Only enable this if your repository is private or you're okay with
  # draft content being visible in version history.
  save_debug_csv: false

  # Linked files backup (PDFs, documents, etc.)
  # Note: Images are ALWAYS downloaded in all common formats (jpg, png, webp, gif, etc.)
  # This setting is ONLY for non-image files like PDFs, EPUBs, ZIPs, etc.
  linked_files:
    # Set to true to enable downloading of linked files
    enabled: true
    # Only download files with these extensions (case-insensitive)
    allowed_extensions:
      - pdf
      - epub
      - zip
    # Only download from these domains (for security)
    allowed_domains:
      - "bear-images.sfo2.cdn.digitaloceanspaces.com"
      - "fischr.org"
      - "gaehn.org"

social:
  # Your Mastodon instance URL
  mastodon_instance: "https://mastodon.social"

  # Maximum age of articles (in days) that will be automatically posted.
  # Articles older than this will be skipped, even if their URL is new.
  # This prevents accidental re-posts when editing old articles (e.g., changing URLs).
  # Set to 0 or remove to disable this check (post all articles regardless of age).
  max_article_age_days: 3

  # Retry queue configuration for partial posting failures
  retry_queue:
    # Retry delays in hours for each attempt (supports decimals for minutes/seconds)
    # Example: [0.5, 2, 6, 24] = retry after 30min, 2h, 6h, 24h
    retry_delays_hours: [1, 4, 12]

    # Maximum number of retry attempts before marking as exhausted
    # Set to 0 to disable retries (will immediately create GitHub issue)
    max_retries: 3

web_archive:
  # Automatically submit new article URLs to the Internet Archive (web.archive.org)
  # This creates a permanent snapshot of your content for long-term preservation
  enabled: true

webmentions:
  # Collect webmentions from blog posts linking to your articles
  # Uses webmention.io service to track traditional blog mentions
  # NOTE: Social media mentions (Mastodon, Bluesky) are excluded as they're
  # already tracked in mappings.json by the social bot
  enabled: true

  # Excluded social media domains (automatically filtered out)
  # These are already tracked in mappings.json
  excluded_domains:
    - mastodon.social
    - bsky.app
    - twitter.com
    - x.com
    - threads.net
    - linkedin.com
    - facebook.com
    - instagram.com
    # Add other Mastodon instances as needed
    - fosstodon.org
    - hachyderm.io
    - mas.to
    - mstdn.social
    - infosec.exchange

link_checker:
  # Enable the broken link checker
  enabled: true
  # HTTP timeout per link (seconds)
  timeout_seconds: 10
  # Concurrent link checks
  max_workers: 8
  # User-Agent used for link checks
  # Using a standard browser User-Agent to avoid being blocked by overly aggressive bot detection
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
  # Domains to exclude from link checking (e.g., sites with aggressive rate limiting)
  excluded_domains:
    - bergfex.de
    - www.bergfex.de
    - bergfex.com
    - www.bergfex.com
    - bergfex.at
    - www.bergfex.at
    - tollwood.de
    - www.tollwood.de
